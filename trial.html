<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam Face Detection</title>
    <style>
        /* Basic styling */
        .video-container {
            width: 640px;
            margin: 0 auto;
            position: relative;
        }

        video {
            width: 100%;
            height: auto;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>

<div class="video-container" id="webcamContainer">
    <video id="webcam" autoplay muted></video>
</div>

<!-- Include face-api.js -->
<script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>

<script>
    document.addEventListener('DOMContentLoaded', async function () {
        const video = document.getElementById('webcam');

        // Load face-api.js models
        await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
        await faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');

        async function startWebcam() {
            try {
                // Request access to the user's webcam
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (error) {
                console.error('Error accessing webcam:', error);
            }
        }

        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video);
            document.getElementById('webcamContainer').append(canvas);
            const displaySize = { width: video.width, height: video.height };
            faceapi.matchDimensions(canvas, displaySize);

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceExpressions();

                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                faceapi.draw.drawDetections(canvas, resizedDetections);
                faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
                faceapi.draw.drawFaceExpressions(canvas, resizedDetections);

                if (detections.length > 0) {
                    const isDistracted = checkForDistraction(detections[0].expressions);
                    if (isDistracted) {
                        alert('Please focus on your study!');
                    }
                }
            }, 1000);
        });

        startWebcam();
    });

    // Function to check if the student is distracted based on facial expressions
    function checkForDistraction(expressions) {
        const distractionThreshold = 0.6;
        return expressions.neutral < distractionThreshold || 
               expressions.surprised > 0.5 ||
               expressions.happy > 0.5 || 
               expressions.sad > 0.5;
    }
</script>

</body>
</html>
